{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyONBapFKiuIDpOMylSBk4Wq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danijel3/CroatianSpeech/blob/main/LM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of making an n-gram language model\n",
        "\n",
        "The purpose of this document is to show the methodology used to produce the n-gram model used in other notebooks.\n",
        "\n",
        "The tool used is [SRILM](http://www.speech.sri.com/projects/srilm/). This tool is very popular in ASR circles, although the license is slightly less conevinient than other tools.\n",
        "\n",
        "Some other LM toolkits available:\n",
        "\n",
        "* [MITLM](https://github.com/mitlm/mitlm) - very small, simple and easy to use, although a bit outdated and simplistic compared to others\n",
        "- [KenLM](https://github.com/kpu/kenlm) - popular for its speed and efficieny, especially for large amounts of data\n",
        "- [IRSTLM](https://github.com/irstlm-team/irstlm) - said to be more accurate in certain cases\n",
        "- [PocoLM](https://github.com/danpovey/pocolm) - made by author(s) of Kaldi, specifically for their ASR - said to be better at fine-tuning to specific domains (see [motivation](https://github.com/danpovey/pocolm/blob/master/docs/motivation.md))\n",
        "\n",
        "I will also recommend the [CMU Sphinx](https://cmusphinx.github.io/wiki/tutoriallm/) manual as a very approachable tutorial for someone starting out with this technique.\n",
        "\n",
        "To use SRILM on your computer, you can either visit the website above and follow their instructions there, or if you are a user of Kaldi, you can use their excellent script in `./tools/extras/install_srilm.sh`.\n",
        "\n",
        "For use in Colab, I have compiled a [version](https://github.com/danijel3/ASRforNLP/releases/tag/v1.0) of SRILM together with all other Kaldi tools that work in Colab. The script to unpack/install them is as follows (note, no GPU is needed for this notebook):"
      ],
      "metadata": {
        "id": "MRD3ob__X7Lt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnAm3Vn7X4oA",
        "outputId": "5e06b3ed-bc55-452f-e6b8-75eef11be688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-24 21:57:50--  https://github.com/danijel3/ASRforNLP/releases/download/v1.0/kaldi.tar.xz\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/409506444/525a8238-abb3-4b8b-8282-12b094577f0e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211224%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211224T215750Z&X-Amz-Expires=300&X-Amz-Signature=29ccfd0c2a7f7bae67342d88c511920b0fee9c5859d656d662bcbc905eba56b6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=409506444&response-content-disposition=attachment%3B%20filename%3Dkaldi.tar.xz&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-12-24 21:57:50--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/409506444/525a8238-abb3-4b8b-8282-12b094577f0e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211224%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211224T215750Z&X-Amz-Expires=300&X-Amz-Signature=29ccfd0c2a7f7bae67342d88c511920b0fee9c5859d656d662bcbc905eba56b6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=409506444&response-content-disposition=attachment%3B%20filename%3Dkaldi.tar.xz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 329172316 (314M) [application/octet-stream]\n",
            "Saving to: ‘kaldi.tar.xz’\n",
            "\n",
            "kaldi.tar.xz        100%[===================>] 313.92M   112MB/s    in 2.8s    \n",
            "\n",
            "2021-12-24 21:57:53 (112 MB/s) - ‘kaldi.tar.xz’ saved [329172316/329172316]\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/danijel3/ASRforNLP/releases/download/v1.0/kaldi.tar.xz\n",
        "\n",
        "!tar xvf kaldi.tar.xz -C / > /dev/null\n",
        "%rm kaldi.tar.xz\n",
        "\n",
        "!for f in $(find /opt/kaldi -name *.so*) ; do ln -sf $f /usr/local/lib/$(basename $f) ; done\n",
        "!for f in $(find /opt/kaldi/src -not -name *.so* -type f -executable) ; do ln -s $f /usr/local/bin/$(basename $f) ; done\n",
        "!for f in $(find /opt/kaldi/tools -not -name *.so* -type f -executable) ; do ln -s $f /usr/local/bin/$(basename $f) ; done\n",
        "\n",
        "!ldconfig"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LM basics\n",
        "\n",
        "Let's take a simple \"text corpus\":"
      ],
      "metadata": {
        "id": "JA5rR_Lbb2Qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile simple.txt\n",
        "ana ide u školu\n",
        "ivo ide u školu\n",
        "ivo ide kući"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTw7317Nb1lB",
        "outputId": "0489d72e-2f9f-4f14-90e7-944f110f2188"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing simple.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making a simple LM from a text file uses the `ngram-count` program (you can find a [complete manual](http://www.speech.sri.com/projects/srilm/manpages/ngram-count.1.html) online):"
      ],
      "metadata": {
        "id": "mdkblFQxcxMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ngram-count -text simple.txt -order 3 -wbdiscount -lm simple.arpa"
      ],
      "metadata": {
        "id": "J-OlLOoycwNX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above command takes the following arguments:\n",
        "* `text` - input text corpus - one sentence per line\n",
        "* `order` - the \"n\" in n-gram, ie. trigram in this case\n",
        "* `wbdiscount` - Witten-Bell discounting method - this one is not the best method, but better ones fail for very small text files\n",
        "* `lm` - output language model file\n",
        "\n",
        "The output file looks as follow:"
      ],
      "metadata": {
        "id": "Ilslw_Z4dKk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cat simple.arpa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLVE0f0vdJZf",
        "outputId": "1745d79d-bbd8-4831-dc05-59d582e836ed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\\data\\\n",
            "ngram 1=8\n",
            "ngram 2=9\n",
            "ngram 3=3\n",
            "\n",
            "\\1-grams:\n",
            "-0.7201593\t</s>\n",
            "-99\t<s>\t-0.2798407\n",
            "-1.021189\tana\t-0.2092596\n",
            "-0.7201593\tide\t-0.2798407\n",
            "-0.8450981\tivo\t-0.3853509\n",
            "-1.021189\tkući\t-0.2092596\n",
            "-0.8450981\tu\t-0.4101745\n",
            "-0.8450981\tškolu\t-0.3853509\n",
            "\n",
            "\\2-grams:\n",
            "-0.69897\t<s> ana\n",
            "-0.39794\t<s> ivo\t0\n",
            "-0.30103\tana ide\n",
            "-0.69897\tide kući\n",
            "-0.39794\tide u\t0\n",
            "-0.1760913\tivo ide\n",
            "-0.30103\tkući </s>\n",
            "-0.1760913\tu školu\t0\n",
            "-0.1760913\tškolu </s>\n",
            "\n",
            "\\3-grams:\n",
            "-0.1760913\t<s> ivo ide\n",
            "-0.1760913\tide u školu\n",
            "-0.1760913\tu školu </s>\n",
            "\n",
            "\\end\\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ARPA-LM is a standrad format with a very long history. It starts with a hedaer beginning from a `/data/` token and counts of individual n-grams present in the file. Following that are lists of all the n-gram orders. You will note that a 3-gram model contains both unigrams and bigrams as well as trigrams, following the standard back-off methodology. \n",
        "\n",
        "Each n-gram line in the file contains two or three fields separated by tab characters (`\\t`):\n",
        "* probability of the n-gram in log (base 10) scale\n",
        "* the name of the n-gram (ie. tokens/words separated by space)\n",
        "* optional back-off weight, also log scale\n",
        "\n",
        "Backing off allows us to acquire a probability value for a sequence of words, even if the n-gram isn't present in the model (eg. because it wasn't present in the training data). In such case, we can use a lower order n-gram but also have to apply the back-off weight (kind of as a form of punishment). The highest order n-grams (trigrams in our case) will never have any back-off weights. The algorithm for calculating the probability for any sequence of words is as follows:\n",
        "1. look for the n-gram in the file and if you find it, return its value\n",
        "2. if you dont find it, use the following formula:\n",
        "\\begin{equation}\n",
        "P( word_N | word_{N-1}, word_{N-2}, ...., word_1 ) = \\\\\n",
        "P( word_N | word_{N-1}, word_{N-2}, ...., word_2 ) \\cdot \\text{backoff-weight}(  word_{N-1} | word_{N-2}, ...., word_1 )\n",
        "\\end{equation}\n",
        "3. if you can't find the n-gram of the lower order, then apply the rule recursively, all the way to unigrams (which should always exists, for words in vocabulary)\n",
        "4. n-grams with now back-off weights are assumed to have value 1 (ie. 0 in log-scale)\n",
        "\n",
        "Note, this assumes a colsed-vocabulary, that is the algorithm will fail for any word that is not inside the predefined vocabulary. A way around this is to add a special \"UNK\" token for any word not in the vocabulary. These are known as \"out-of-vocabulary\" words, or OOV. Modelling OOVs in n-grams does work, but obviously has its limitations (eg. one weight for any OOV for starters).\n",
        "\n",
        "GOing back to the algorithm for a sec, lets takt a sequence `ana ide`. The probability of that would be:\n",
        "\n",
        "\\begin{equation}\n",
        "P(ide|ana) = 10^{-0.30103} = 0.4999999950079739\n",
        "\\end{equation}\n",
        "\n",
        "The probability of `ana ide kući`:\n",
        "\n",
        "\\begin{equation}\n",
        "P(kući|ana,ide) = P(kući|ide)*bwt(ide|ana)=10^{(-0.69897+0)}=0.20000000199681048\n",
        "\\end{equation}\n",
        "\n",
        "To verify these calculations, we can use the excellent python [ARPA](https://pypi.org/project/arpa/) library. This library contains no model preparation code, but can load any ARPA file and apply it to any n-gram or sentence. It is installed using pip:"
      ],
      "metadata": {
        "id": "Gg6QveNLd5x3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arpa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4lf0264h31Q",
        "outputId": "df30fcb9-a2d4-413e-8368-c8aab777866f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arpa\n",
            "  Downloading arpa-0.1.0b4-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: arpa\n",
            "Successfully installed arpa-0.1.0b4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can compute the above values like so:"
      ],
      "metadata": {
        "id": "aqgEw4QJjabw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import arpa\n",
        "\n",
        "lm=arpa.loadf('simple.arpa')[0]\n",
        "\n",
        "print(lm.p('ana ide'))\n",
        "print(lm.p('ana ide kući'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mzgu-1dgjZhr",
        "outputId": "61640d86-23a2-422d-9e5b-0684ba3a24e4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4999999950079739\n",
            "0.20000000199681048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Larger LM\n",
        "\n",
        "Let's create a mode realistic LM using a slightly bigger dataset:"
      ],
      "metadata": {
        "id": "5VudNlxKj-zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/danijel3/CroatianSpeech/raw/main/ParlaMint-HR_S01.text.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1YVzdIXjuyg",
        "outputId": "de0a0c42-05f4-42f7-b310-c5a6ec3b4e5f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-24 22:30:11--  https://github.com/danijel3/CroatianSpeech/raw/main/ParlaMint-HR_S01.text.txt\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/danijel3/CroatianSpeech/main/ParlaMint-HR_S01.text.txt [following]\n",
            "--2021-12-24 22:30:12--  https://raw.githubusercontent.com/danijel3/CroatianSpeech/main/ParlaMint-HR_S01.text.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4012131 (3.8M) [text/plain]\n",
            "Saving to: ‘ParlaMint-HR_S01.text.txt’\n",
            "\n",
            "ParlaMint-HR_S01.te 100%[===================>]   3.83M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-12-24 22:30:12 (57.8 MB/s) - ‘ParlaMint-HR_S01.text.txt’ saved [4012131/4012131]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time we will use a slightly better discounting method (at least what people in ASR community found to be better in most cases). We will also use the interpolate function to tweak the weights a bit and add the UNK token for OOVs:"
      ],
      "metadata": {
        "id": "aXKBEWClkTGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ngram-count -text ParlaMint-HR_S01.text.txt -order 3 -kndiscount -interpolate -unk -lm hrvatski.arpa"
      ],
      "metadata": {
        "id": "j_EKD5MNkNra"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When dealing with larger LMs, it is standard practice to compress the files. Since this is a text format, compression helps a lot. SRILM tools work fine with compressed files, as do many other toolkits, although they may have to have this feature added during compilation (eg. KenLM does). The PyTorch decoder does not support compressed LMs, afaik:"
      ],
      "metadata": {
        "id": "SfJQ9hVJlWie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip hrvatski.arpa"
      ],
      "metadata": {
        "id": "0X-9WADYlUZU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One thing that is standard practice with making these models is computing perplexity. This is a standard positive, but unconstrained measure, with lower values being better. To compute the value, we apply the model on an independent evaluation text, for example let's take this short output from our ASR:"
      ],
      "metadata": {
        "id": "URkNAAVEl7kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile eval.txt\n",
        "potpredsjedniče poštovane kolegice i kolege ovo je još jedan od zakona koji raspravljamo ovih dana ili ćemo raspravljati narednih dana koji su možda imali dobar motiv da budu upućeni hrvatskom saboru ali"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojSdD_jRl4-H",
        "outputId": "b17c4714-618c-4c4c-efe8-c43405ad37fb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing eval.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the `ngram` program to apply various computations on an already existsing LM:"
      ],
      "metadata": {
        "id": "6JFI1_atmtBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ngram -lm hrvatski.arpa.gz -unk -ppl eval.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJi0lWddmrdg",
        "outputId": "7b4ca420-0637-46f7-c08e-5f59fb03237a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file eval.txt: 1 sentences, 32 words, 0 OOVs\n",
            "0 zeroprobs, logprob= -76.50246 ppl= 208.0924 ppl1= 245.8679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command gave us to perplexity values: `ppl` and `ppl1`. The former is calculated only using the given words, and the latter is calculated by also applying the sentence boundary marks (ie. `<s>` and `</s>`). Whichever you use is totally up to you, but usually the choice doesn't matter much. \n",
        "\n",
        "The way we would use this information is if we change something in how the model is trained (eg. we use different parameters in the `ngram-count` command above, or we change the training data somehow), we can use PPL to compare the models and see which one is better.\n",
        "\n",
        "Note that normally this program also calculates OOV, but for models that contain the UNK token, this value will unfortunately always be 0. This isn't a problem to count yourself, using a separate program/script, and is generally recommended as OOVs will often be a very big factor in WER.\n",
        "\n",
        "Finally, we can use the `ngram` program to do fun stuff, like generate random sentences from the LM:"
      ],
      "metadata": {
        "id": "YyD3D00Pm80Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ngram -lm hrvatski.arpa.gz -unk -gen 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-o5FQ8rm7DX",
        "outputId": "16a21c6d-1756-4f34-afd1-54ddcab65052"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kakve Marasu, bili danas ako ste vi tako jednom tretman gdje rješenja bilo njihova javne dužnosti.\n",
            "Hvala potpredsjedniče Hrvatskog sabora.\n",
            "Ja ću i ruralnih „Isušiti močvaru“. predsjednik. ima ovrhu gospodarstva, radimo ova dva dana samo što će zakon.\n",
            "Idući sustava, situaciju reforme i HSU-a, smije koji to znači stavak 1., prema toj identificirate jako svinjogojstvu, pomorske snage i godinu dana modela i ostvariti iskreno sumnjamo. počešali. klubova to ne riješe dovede u BiH, da onaj tko je bilo.\n",
            "Zahvaljujem zastupniku smo u Europskom novine nije.\n",
            "i to je da on nije uvjeravam stranih onda se radi.\n",
            "To su vrlo ne ostvarena i niste mogli imati ogroman srpskom dajte pred proširen katalog kaznenih djela uvaženog grada, facebooku korištenje europskih dakle u sadašnjost.\n",
            "Pa stimulativno djelovala. stanovništva a i osobe se da su manje i sustav koji će uslijediti ili tako unaprijedite i čitav niz pitanja ako se stalo nepromišljeno nelogičnosti kod nas izabrale.\n",
            "Poštovani predsjedniče Hrvatskog sabora.\n",
            "Pa kad Socijalističke farmaceutskim ova tema jer ste vi predlagali odbora.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final words\n",
        "\n",
        "This is where this small document ends for now. The above set of random sentences shows pretty well what the limitations of this method is. To fully control this technique, several more topics are recommended for in-depth study:\n",
        "\n",
        "* various discounting and interpolation methods and their meaning for the final result\n",
        "* managing size and complexity of models, eg. by pruning\n",
        "* considering the combination of char-level and LM-based decoding in E2E systems (the decoder has a weight which allows to consider either option to a certaing degree)\n",
        "* dealing with [large-scale LMs](https://cmusphinx.github.io/wiki/tutoriallmadvanced/), model mixing and domain adaptation\n",
        "* other advanced n-gram techniques like: skip-grams, class-based LMs, factored LMs, hidden event LMs, sub-word modeling, etc..."
      ],
      "metadata": {
        "id": "W7oIBdQFoqKh"
      }
    }
  ]
}